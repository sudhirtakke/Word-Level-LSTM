{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "WordLevel LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sudhirtakke/Word-Level-LSTM/blob/main/WordLevel_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBO78ScK3FFx"
      },
      "source": [
        "# Building new dialogues using Keras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dOj1B8d3FFz"
      },
      "source": [
        "### Table of Contents\n",
        "\n",
        "1. [Learning Goals](#section1)\n",
        "2. [Language Model Design](#section2)\n",
        "3. [Load Text](#section3)\n",
        "4. [Clean Text](#section4)\n",
        "5. [Save Cleaned Text](#section5)\n",
        "6. [Train Language Model](#section6)\n",
        " - a. [Load Sequences](#section601)\n",
        " - b. [Encode Sequences](#section602)\n",
        " - c. [Sequence Inputs and Output](#section603)\n",
        " - d. [Fit Model](#section604)\n",
        " - e. [Save the model](#section605)\n",
        "7. [Use Language model](#section7)\n",
        " - a. [Load the Data](#section701)\n",
        " - b. [Load Model](#section702)\n",
        " - c. [Generate Text](#section703)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-oYCORA4oCf",
        "toc-hr-collapsed": false
      },
      "source": [
        "\n",
        "<br>\n",
        "\n",
        "* We are going to develop **word-level neural language model** and use it to generate text.\n",
        "\n",
        "* A **language model** can predict the probability of the next word in the sequence, based on the **words already observed** in the sequence.\n",
        "\n",
        "* **Neural network models** are a preferred method for **developing statistical language models** because they can use a **distributed representation** where different words with similar meanings have **similar representation**.\n",
        "\n",
        "- Also, it is because they can use a **large context** of recently observed words when **making predictions**.\n",
        "\n",
        "\n",
        "\n",
        "<a id=section1></a>\n",
        "## 1. Learning goals\n",
        " \n",
        "\n",
        "* How to prepare text for developing a **word-based language** model ?\n",
        "* How to design and fit a **neural language model** with a **learned embedding** and an **LSTM hidden layer** ?\n",
        "* How to use the **learned language model** to generate **new text** with **similar statistical properties** as the source text ?\n",
        "\n",
        "### Overview\n",
        "1. The Republic by Plato\n",
        "2. Data Preparation\n",
        "3. Train Language Model\n",
        "4. Use Language Model\n",
        "\n",
        "---\n",
        "\n",
        "## The Republic by Plato\n",
        "<br>\n",
        "\n",
        "- Download the ASCII **text version** of the entire book (or books) here: [The Republic](https://https://www.gutenberg.org/ebooks/1497) and save it as *republic.txt*\n",
        "\n",
        "- **Open the file in a text editor and delete the front and back matter. This includes details about the book at the beginning, a long analysis, and license information at the end.**\n",
        "\n",
        "## Data Preparation\n",
        "\n",
        "- We will start by **preparing the data** for modeling.\n",
        "\n",
        "- The first step is to look at the data.\n",
        "\n",
        "### Review the Text\n",
        "- Open the text in an editor and just look at the text data.\n",
        "\n",
        "- For example, here is the first piece of dialog:\n",
        "\n",
        "> BOOK I.\n",
        "\n",
        "        I went down yesterday to the Piraeus with Glaucon the son of Ariston,\n",
        "        that I might offer up my prayers to the goddess (Bendis, the Thracian\n",
        "        Artemis.); and also because I wanted to see in what manner they would\n",
        "        celebrate the festival, which was a new thing. I was delighted with the\n",
        "        procession of the inhabitants; but that of the Thracians was equally,\n",
        "        if not more, beautiful. When we had finished our prayers and viewed the\n",
        "        spectacle, we turned in the direction of the city; and at that instant\n",
        "        Polemarchus the son of Cephalus chanced to catch sight of us from a\n",
        "        distance as we were starting on our way home, and told his servant to\n",
        "        run and bid us wait for him. The servant took hold of me by the cloak\n",
        "        behind, and said: Polemarchus desires you to wait.\n",
        "\n",
        "        I turned round, and asked him where his master was.\n",
        "\n",
        "        There he is, said the youth, coming after you, if you will only wait.\n",
        "\n",
        "        Certainly we will, said Glaucon; and in a few minutes Polemarchus\n",
        "        appeared, and with him Adeimantus, Glaucon’s brother, Niceratus the son\n",
        "        of Nicias, and several others who had been at the procession.\n",
        "\n",
        "        Polemarchus said to me: I perceive, Socrates, that you and your\n",
        "        companion are already on your way to the city.\n",
        "\n",
        "        You are not far wrong, I said.\n",
        "        ...\n",
        "        \n",
        "### Here’s what we see from a quick look:\n",
        "\n",
        "* Book/Chapter headings (e.g. “BOOK I.”).\n",
        "* British English spelling (e.g. “honoured”)\n",
        "* Lots of punctuation (e.g. “–“, “;–“, “?–“, and more)\n",
        "* Strange names (e.g. “Polemarchus”).\n",
        "* Some long monologues that go on for hundreds of lines.\n",
        "* Some quoted dialog (e.g. ‘…’)\n",
        "\n",
        "\n",
        "<a id=section2></a>\n",
        "### 2. PLAN:  Language model design\n",
        "\n",
        "- It will be **statistical** and will **predict the probability** of **each word** given an input sequence of text.\n",
        "- The **predicted word** will be fed in as input to in turn **generate the next word**.\n",
        "\n",
        "- A key design decision is how long the **input sequences** should be.\n",
        "- They need to be long enough to allow the model to **learn the context** for the words to predict.\n",
        "- This **input length** will also define the **length of seed text** used to generate **new sequences** when we use the model.\n",
        "- There is **no correct** answer.\n",
        "- With enough time and resources, we could explore the **ability of the model** to learn with **differently sized input sequences**.\n",
        "\n",
        "- Instead, we will pick a length of **50 words** for the length of the **input sequences**, somewhat arbitrarily."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvysmaT93FF6"
      },
      "source": [
        "We'll be using the following **process sequence** in this notebook:\n",
        "![](https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/word_lstm_flow0.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06Z_9cCHE8-u"
      },
      "source": [
        "<a id=section3></a>\n",
        "### 3. Load Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Np8phFLO3FGA"
      },
      "source": [
        "- The first step is to **load the text** into **memory** as a **sequence** of **loaded text**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItkIWd-QuyJn"
      },
      "source": [
        "# Import tensorflow 2.x\n",
        "# This code block will only work in Google Colab.\n",
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gq0Ve7ItvsQP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef2cfa3b-a448-41a9-ec03-a106de9877a1"
      },
      "source": [
        "import urllib\n",
        "import requests\n",
        "response = urllib.request.urlopen('https://raw.githubusercontent.com/insaid2018/DeepLearning/master/Data/republic_clean.txt')\n",
        "doc = response.read().decode('utf8')\n",
        "print(doc[:200])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "﻿\rBOOK I.\r\r\n",
            "\r\r\n",
            "I went down yesterday to the Piraeus with Glaucon the son of Ariston,\r\r\n",
            "that I might offer up my prayers to the goddess (Bendis, the Thracian\r\r\n",
            "Artemis.); and also because I wanted to s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkV_4Y0SGNL1"
      },
      "source": [
        "<a id=section4></a>\n",
        "### 4. Clean Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKSbwDDO3FGU"
      },
      "source": [
        "- We need to transform the **raw text** into a **sequence of tokens** or words that we can use as a source to train the model.\n",
        "\n",
        "- Based on **reviewing the raw text** (above), below are some specific operations we will perform to clean the text. \n",
        "- We may want to explore **more cleaning operations** as an extension.\n",
        "\n",
        "* **Replace ‘–‘** with a white space so we can split words better.\n",
        "* **Split words** based on **white space**.\n",
        "* Remove all **punctuation** from **words** to reduce the vocabulary size (e.g. ‘What?’ becomes ‘What’).\n",
        "* **Remove all words** that are not alphabetic to remove standalone **punctuation tokens**.\n",
        "* Normalize **all words** to **lowercase** to reduce the **vocabulary size**.\n",
        "\n",
        "\n",
        "- **Vocabulary size** is a big deal with language modeling.\n",
        "- A **smaller vocabulary** results in a **smaller model** that **trains faster.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCORHQHPGmTC"
      },
      "source": [
        "import string\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "    # replace '--' with a space ' '\n",
        "    doc = doc.replace('--', ' ')\n",
        "    # split into tokens by white space\n",
        "    tokens = doc.split()\n",
        "    # remove punctuation from each token\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    tokens = [w.translate(table) for w in tokens]\n",
        "    # remove remaining tokens that are not alphabetic\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    # make lower case\n",
        "    tokens = [word.lower() for word in tokens]\n",
        "    return tokens"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPb4XvVCGslQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2502d1b-d374-4ab4-c11b-beb7e763f7bc"
      },
      "source": [
        "# clean document\n",
        "tokens = clean_doc(doc)\n",
        "print(tokens[:200])\n",
        "print('Total Tokens: %d' % len(tokens))\n",
        "print('Unique Tokens: %d' % len(set(tokens)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['book', 'i', 'i', 'went', 'down', 'yesterday', 'to', 'the', 'piraeus', 'with', 'glaucon', 'the', 'son', 'of', 'ariston', 'that', 'i', 'might', 'offer', 'up', 'my', 'prayers', 'to', 'the', 'goddess', 'bendis', 'the', 'thracian', 'artemis', 'and', 'also', 'because', 'i', 'wanted', 'to', 'see', 'in', 'what', 'manner', 'they', 'would', 'celebrate', 'the', 'festival', 'which', 'was', 'a', 'new', 'thing', 'i', 'was', 'delighted', 'with', 'the', 'procession', 'of', 'the', 'inhabitants', 'but', 'that', 'of', 'the', 'thracians', 'was', 'equally', 'if', 'not', 'more', 'beautiful', 'when', 'we', 'had', 'finished', 'our', 'prayers', 'and', 'viewed', 'the', 'spectacle', 'we', 'turned', 'in', 'the', 'direction', 'of', 'the', 'city', 'and', 'at', 'that', 'instant', 'polemarchus', 'the', 'son', 'of', 'cephalus', 'chanced', 'to', 'catch', 'sight', 'of', 'us', 'from', 'a', 'distance', 'as', 'we', 'were', 'starting', 'on', 'our', 'way', 'home', 'and', 'told', 'his', 'servant', 'to', 'run', 'and', 'bid', 'us', 'wait', 'for', 'him', 'the', 'servant', 'took', 'hold', 'of', 'me', 'by', 'the', 'cloak', 'behind', 'and', 'said', 'polemarchus', 'desires', 'you', 'to', 'wait', 'i', 'turned', 'round', 'and', 'asked', 'him', 'where', 'his', 'master', 'was', 'there', 'he', 'is', 'said', 'the', 'youth', 'coming', 'after', 'you', 'if', 'you', 'will', 'only', 'wait', 'certainly', 'we', 'will', 'said', 'glaucon', 'and', 'in', 'a', 'few', 'minutes', 'polemarchus', 'appeared', 'and', 'with', 'him', 'adeimantus', 'glaucons', 'brother', 'niceratus', 'the', 'son', 'of', 'nicias', 'and', 'several', 'others', 'who', 'had', 'been', 'at', 'the', 'procession', 'polemarchus', 'said']\n",
            "Total Tokens: 118684\n",
            "Unique Tokens: 7409\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9bm-2ZyGy-q"
      },
      "source": [
        "<a id=section5></a>\n",
        "### 5. Save clean text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xf7dLooF3FGp"
      },
      "source": [
        "- We can organize the long list of tokens into sequences of **50 input words** and **1 output word**.\n",
        "\n",
        "- That is, sequences of **51 words**.\n",
        "\n",
        "- We can do this by iterating over the list of tokens from token 51 onwards and taking the prior **50 tokens as a sequence**, then repeating this process to the end of the list of tokens.\n",
        "\n",
        "- We will transform the tokens into **space-separated strings** for later storage in a file.\n",
        "\n",
        "- The code to split the list of **clean tokens** into **sequences with a length of 51 tokens** is listed below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOI6sfwaG8QB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3c7da9e-3ba4-4cd6-c807-c6e3740e5b4b"
      },
      "source": [
        "# organize into sequences of tokens\n",
        "length = 50 + 1\n",
        "sequences = list()\n",
        "for i in range(length, len(tokens)):\n",
        "    # select sequence of tokens\n",
        "    seq = tokens[i-length:i]\n",
        "    # convert into a line\n",
        "    line = ' '.join(seq)\n",
        "    # store\n",
        "    sequences.append(line)\n",
        "print('Total Sequences: %d' % len(sequences))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Sequences: 118633\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5YCAlga3FGw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9957d0e0-040e-4182-fbed-0dbd02bb110a"
      },
      "source": [
        "sequences[:2]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['book i i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was',\n",
              " 'i i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RksKge0_G_Oj"
      },
      "source": [
        "# save tokens to file, one dialog per line\n",
        "def save_doc(lines, filename):\n",
        "    data = '\\n'.join(lines)\n",
        "    file = open(filename, 'w')\n",
        "    file.write(data)\n",
        "    file.close()\n",
        "    \n",
        "# save sequences to file\n",
        "out_filename = 'republic_sequences.txt'\n",
        "save_doc(sequences, out_filename)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IwlwV4_H3OW"
      },
      "source": [
        "<a id=section6></a>\n",
        "## 6. Train Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHCFMESq3FG7"
      },
      "source": [
        "We can now train a **statistical language mode**l from the prepared data.\n",
        "\n",
        "The model we will train is a neural language model. It has a few unique characteristics:\n",
        "\n",
        "* It uses a **distributed representation for words** so that different words with similar meanings will have a similar representation.\n",
        "* It **learns** the **representation** at the same time as **learning the model.**\n",
        "* It **learns** to **predict the probability** for the next word using the context of the last 100 words.\n",
        "\n",
        "Specifically, we will use an **Embedding Layer** to learn the representation of words, and a **Long Short-Term Memory (LSTM)** recurrent neural network to learn to **predict words** based on their context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rL1mwuBc3FG-"
      },
      "source": [
        "### Let’s start by loading our training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NmWD7bb3FG_"
      },
      "source": [
        "<a id=section601></a>\n",
        "### a. Load Sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBgoos1p3FHB"
      },
      "source": [
        "- We can load our **training data** using the **`load_doc()`** function defined below.\n",
        "\n",
        "\n",
        "- Once loaded, we can **split the data into separate training sequences** by splitting based on new lines.\n",
        "\n",
        "\n",
        "- The snippet below will load the **‘republic_sequences.txt‘** data file from the current working directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYZIaBv-IRDJ"
      },
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, 'r')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "# load\n",
        "in_filename = 'republic_sequences.txt'\n",
        "doc = load_doc(in_filename)\n",
        "lines = doc.split('\\n')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7kz_wjL3FHH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac3b83fe-f015-46c8-ce7a-87e4497b43c0"
      },
      "source": [
        "lines[:2]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['book i i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was',\n",
              " 'i i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhFPp4lxIW1E"
      },
      "source": [
        "<a id=section602></a>\n",
        "### b. Encode Sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdcRYdZI3FHN"
      },
      "source": [
        "- The **word embedding layer** expects input sequences to be comprised of integers.\n",
        "\n",
        "- We can **map each word in our vocabulary** to a unique integer and encode our input sequences.\n",
        "- Later, when we make predictions, we can convert the **prediction to numbers** and look up their **associated words** in the **same mapping**.\n",
        "\n",
        "- To do this **encoding**, we will use the **`Tokenizer`** class in the Keras API.\n",
        "\n",
        "- First, the **Tokenizer** must be trained on the **entire training dataset**, which means it finds all of the unique words in the data and assigns each a unique integer.\n",
        "\n",
        "- We can then use the **fit Tokenizer** to encode all of the training sequences, **converting each sequence** from a **list of words** to a **list of integers**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDn6Si02IeI6"
      },
      "source": [
        "from numpy import array\n",
        "from pickle import dump\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "# integer encode sequences of words\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(lines)\n",
        "sequences = tokenizer.texts_to_sequences(lines)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxR_DnR9I7Zt"
      },
      "source": [
        "- We can access the **mapping of words to integers** as a dictionary attribute called **word_index on the Tokenizer object**.\n",
        "\n",
        "- We need to know the **size of the vocabulary** for defining the embedding layer later. \n",
        "- We can determine the vocabulary by **calculating the size** of the **mapping dictionary**.\n",
        "\n",
        "- Words are assigned values from **1 to the total number of words** (e.g. 7,409).\n",
        "- The **Embedding layer** needs to allocate a **vector representation** for each word in this vocabulary from **index 1 to the largest index** \n",
        "- It is because indexing of arrays is **zero-offset**, the index of the word at the end of the vocabulary will be 7,409\n",
        "- This means the array must be **7,409 + 1** in length.\n",
        "\n",
        "- Therefore, when specifying the **vocabulary size** to the **Embedding layer**, we specify it as 1 larger than the **actual vocabulary**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2ZZJ3DxI_2-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4fb7b94-8544-4840-ad83-c62d0d72dd1f"
      },
      "source": [
        "# vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "vocab_size"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7410"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWagP5ehJCGL"
      },
      "source": [
        "<a id=section603></a>\n",
        "### c. Sequence Inputs and Output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0lorZEM3FHZ"
      },
      "source": [
        "- Now that we have encoded the input sequences, we need to separate them into input (X) and output (y) elements.\n",
        "\n",
        "- We can do this with **array slicing**.\n",
        "\n",
        "- After separating, we need to **one hot encode the output word**. \n",
        "- This means converting it from an integer to a vector of 0 values, one for each word in the vocabulary, with a **1 to indicate the specific word** at the index of the words integer value.\n",
        "\n",
        "- This is so that the model learns to **predict the probability distribution** for the next word and the ground truth from which to learn from is 0 for all words except the actual word that comes next.\n",
        "\n",
        "- Keras provides the **to_categorical()** that can be used to **one hot encode** the output words for each **input-output sequence** pair.\n",
        "\n",
        "- Finally, we need to specify to the **Embedding layer** how long input sequences are. \n",
        "- We know that there are **50 words** because we designed the model, but a good generic way to specify that is to use the **second dimension (number of columns)** of the input data’s shape. \n",
        "- That way, if We change the **length of sequences** when preparing data, We do not need to change this **data loading code**; it is generic."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PACdSAPYJAbT",
        "scrolled": true
      },
      "source": [
        "# separate into input and output\n",
        "sequences = array(sequences)\n",
        "X, y = sequences[:,:-1], sequences[:,-1]\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "seq_length = X.shape[1]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugQoKt9G3FHe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94e1af60-f405-4b65-bf37-f98bf5dbac11"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(118633, 50)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WbqBTEc3FHm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7a011cd-0e12-499a-d25e-fa3bb66907df"
      },
      "source": [
        "X[0]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1046,   11,   11, 1045,  329, 7409,    4,    1, 2873,   35,  213,\n",
              "          1,  261,    3, 2251,    9,   11,  179,  817,  123,   92, 2872,\n",
              "          4,    1, 2249, 7408,    1, 7407, 7406,    2,   75,  120,   11,\n",
              "       1266,    4,  110,    6,   30,  168,   16,   49, 7405,    1, 1609,\n",
              "         13,   57,    8,  549,  151,   11])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "easmWXJu3FHv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a9a068f-6567-400e-8519-e7dd0118dfd5"
      },
      "source": [
        "y.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(118633, 7410)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9G_pEZAf3FHy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c086314-4bfa-4851-b267-eb39decbb1e8"
      },
      "source": [
        "y[0]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9PvtEQA3FH0"
      },
      "source": [
        "<a id=section604></a>\n",
        "### d. Fit Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zw5tiv7JKwX"
      },
      "source": [
        "- We can now **define and fit** our language model on the training data.\n",
        "\n",
        "- The **learned embedding** needs to know the size of the vocabulary and the **length of input sequences** as previously discussed.\n",
        "\n",
        " - **Size of the embedding vector space**: a parameter to specify how many dimensions will be used to represent each word\n",
        "\n",
        "- Common values are **50, 100, and 300**. \n",
        "- We will use 50 here, but consider **testing smaller or larger values**.\n",
        "\n",
        "- We will use a **two LSTM hidden layers** with **100 memory cells** each. \n",
        "- More **memory cells** and a **deeper network** may achieve better results.\n",
        "\n",
        " \n",
        "###  Procedure:\n",
        "\n",
        " - A **dense fully connected layer** with **100 neurons** connects to the **LSTM hidden layers** to interpret the features extracted from the sequence. \n",
        " - The **output layer** predicts the **next word** as a single vector the **size of the vocabulary** with a probability for each word in the vocabulary. \n",
        " - A **softmax activation functio**n is used to **ensure the outputs** have the characteristics of normalized probabilities.\n",
        " \n",
        " <center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/images.png\"/></center>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmzS7OUuJIps",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7babc3b0-369f-46c2-f015-80d7d69f4ea9"
      },
      "source": [
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
        "model.add(LSTM(100, return_sequences=True))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "print(model.summary())"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 50, 50)            370500    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 50, 100)           60400     \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 7410)              748410    \n",
            "=================================================================\n",
            "Total params: 1,269,810\n",
            "Trainable params: 1,269,810\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-2JH1NlJYcW"
      },
      "source": [
        "-  The model is compiled specifying the **categorical cross entropy loss** needed to fit the model.\n",
        "- Technically, the **model** is learning a **multi-class classification** and this is the suitable loss function for this type of problem. \n",
        "- The efficient **Adam implementation** to **mini-batch gradient descent** is used and accuracy is evaluated of the model.\n",
        "\n",
        "- Finally, the **model** is fit on the data for **100 training epochs** with a modest batch size of 128 to speed things up.\n",
        "\n",
        "- Training may take a **few hours on modern hardware** without GPUs. \n",
        "- We can speed it up with a **larger batch size** and/or fewer training epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrgQUag_JUwW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3f999d9-ed6a-4002-a50f-e86dcc783bec"
      },
      "source": [
        "# compile model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit model\n",
        "model.fit(X, y, batch_size=128, epochs=100)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "927/927 [==============================] - 52s 47ms/step - loss: 6.1543 - accuracy: 0.0729\n",
            "Epoch 2/100\n",
            "927/927 [==============================] - 41s 44ms/step - loss: 5.6757 - accuracy: 0.1084\n",
            "Epoch 3/100\n",
            "927/927 [==============================] - 41s 45ms/step - loss: 5.4392 - accuracy: 0.1334\n",
            "Epoch 4/100\n",
            "927/927 [==============================] - 41s 45ms/step - loss: 5.2873 - accuracy: 0.1448\n",
            "Epoch 5/100\n",
            "927/927 [==============================] - 41s 44ms/step - loss: 5.1735 - accuracy: 0.1534\n",
            "Epoch 6/100\n",
            "927/927 [==============================] - 41s 44ms/step - loss: 5.1607 - accuracy: 0.1536\n",
            "Epoch 7/100\n",
            "927/927 [==============================] - 40s 43ms/step - loss: 5.0420 - accuracy: 0.1614\n",
            "Epoch 8/100\n",
            "927/927 [==============================] - 40s 43ms/step - loss: 4.9616 - accuracy: 0.1664\n",
            "Epoch 9/100\n",
            "927/927 [==============================] - 41s 44ms/step - loss: 4.8878 - accuracy: 0.1709\n",
            "Epoch 10/100\n",
            "927/927 [==============================] - 41s 44ms/step - loss: 4.8144 - accuracy: 0.1735\n",
            "Epoch 11/100\n",
            "927/927 [==============================] - 41s 44ms/step - loss: 4.7405 - accuracy: 0.1784\n",
            "Epoch 12/100\n",
            "927/927 [==============================] - 41s 44ms/step - loss: 4.6600 - accuracy: 0.1828\n",
            "Epoch 13/100\n",
            "927/927 [==============================] - 41s 45ms/step - loss: 4.5842 - accuracy: 0.1852\n",
            "Epoch 14/100\n",
            "927/927 [==============================] - 42s 45ms/step - loss: 4.5113 - accuracy: 0.1889\n",
            "Epoch 15/100\n",
            "927/927 [==============================] - 41s 44ms/step - loss: 4.4377 - accuracy: 0.1914\n",
            "Epoch 16/100\n",
            "927/927 [==============================] - 41s 44ms/step - loss: 4.3672 - accuracy: 0.1951\n",
            "Epoch 17/100\n",
            "927/927 [==============================] - 41s 44ms/step - loss: 4.3138 - accuracy: 0.1969\n",
            "Epoch 18/100\n",
            "927/927 [==============================] - 41s 44ms/step - loss: 4.2376 - accuracy: 0.1997\n",
            "Epoch 19/100\n",
            "927/927 [==============================] - 41s 45ms/step - loss: 4.1767 - accuracy: 0.2033\n",
            "Epoch 20/100\n",
            "927/927 [==============================] - 41s 44ms/step - loss: 4.1881 - accuracy: 0.2028\n",
            "Epoch 21/100\n",
            "927/927 [==============================] - 41s 45ms/step - loss: 4.1584 - accuracy: 0.2082\n",
            "Epoch 22/100\n",
            "927/927 [==============================] - 41s 45ms/step - loss: 4.0778 - accuracy: 0.2118\n",
            "Epoch 23/100\n",
            "927/927 [==============================] - 41s 44ms/step - loss: 4.0133 - accuracy: 0.2156\n",
            "Epoch 24/100\n",
            "927/927 [==============================] - 41s 44ms/step - loss: 3.9537 - accuracy: 0.2191\n",
            "Epoch 25/100\n",
            "927/927 [==============================] - 41s 45ms/step - loss: 3.8967 - accuracy: 0.2237\n",
            "Epoch 26/100\n",
            "927/927 [==============================] - 41s 44ms/step - loss: 3.8434 - accuracy: 0.2280\n",
            "Epoch 27/100\n",
            "927/927 [==============================] - 41s 45ms/step - loss: 3.7889 - accuracy: 0.2331\n",
            "Epoch 28/100\n",
            "927/927 [==============================] - 41s 44ms/step - loss: 3.7393 - accuracy: 0.2387\n",
            "Epoch 29/100\n",
            "927/927 [==============================] - 40s 44ms/step - loss: 3.6899 - accuracy: 0.2442\n",
            "Epoch 30/100\n",
            "927/927 [==============================] - 40s 43ms/step - loss: 3.6397 - accuracy: 0.2480\n",
            "Epoch 31/100\n",
            "927/927 [==============================] - 40s 44ms/step - loss: 3.5945 - accuracy: 0.2536\n",
            "Epoch 32/100\n",
            "927/927 [==============================] - 40s 43ms/step - loss: 3.5494 - accuracy: 0.2598\n",
            "Epoch 33/100\n",
            "927/927 [==============================] - 40s 43ms/step - loss: 3.5038 - accuracy: 0.2649\n",
            "Epoch 34/100\n",
            "927/927 [==============================] - 40s 43ms/step - loss: 3.4608 - accuracy: 0.2702\n",
            "Epoch 35/100\n",
            "927/927 [==============================] - 40s 43ms/step - loss: 3.4168 - accuracy: 0.2753\n",
            "Epoch 36/100\n",
            "927/927 [==============================] - 40s 43ms/step - loss: 3.3756 - accuracy: 0.2807\n",
            "Epoch 37/100\n",
            "927/927 [==============================] - 40s 43ms/step - loss: 3.3349 - accuracy: 0.2867\n",
            "Epoch 38/100\n",
            "927/927 [==============================] - 40s 43ms/step - loss: 3.2937 - accuracy: 0.2915\n",
            "Epoch 39/100\n",
            "927/927 [==============================] - 40s 43ms/step - loss: 3.2570 - accuracy: 0.2972\n",
            "Epoch 40/100\n",
            "927/927 [==============================] - 40s 43ms/step - loss: 3.2189 - accuracy: 0.3034\n",
            "Epoch 41/100\n",
            "927/927 [==============================] - 40s 43ms/step - loss: 3.1810 - accuracy: 0.3085\n",
            "Epoch 42/100\n",
            "927/927 [==============================] - 40s 43ms/step - loss: 3.1440 - accuracy: 0.3132\n",
            "Epoch 43/100\n",
            "927/927 [==============================] - 40s 43ms/step - loss: 3.1087 - accuracy: 0.3188\n",
            "Epoch 44/100\n",
            "927/927 [==============================] - 40s 43ms/step - loss: 3.0723 - accuracy: 0.3247\n",
            "Epoch 45/100\n",
            "927/927 [==============================] - 40s 43ms/step - loss: 3.0396 - accuracy: 0.3283\n",
            "Epoch 46/100\n",
            "927/927 [==============================] - 41s 44ms/step - loss: 3.0037 - accuracy: 0.3344\n",
            "Epoch 47/100\n",
            "927/927 [==============================] - 41s 44ms/step - loss: 2.9712 - accuracy: 0.3400\n",
            "Epoch 48/100\n",
            "927/927 [==============================] - 41s 44ms/step - loss: 2.9379 - accuracy: 0.3460\n",
            "Epoch 49/100\n",
            "927/927 [==============================] - 41s 44ms/step - loss: 2.9062 - accuracy: 0.3510\n",
            "Epoch 50/100\n",
            "927/927 [==============================] - 40s 43ms/step - loss: 2.8753 - accuracy: 0.3553\n",
            "Epoch 51/100\n",
            "927/927 [==============================] - 41s 44ms/step - loss: 2.8457 - accuracy: 0.3601\n",
            "Epoch 52/100\n",
            "927/927 [==============================] - 41s 44ms/step - loss: 2.8145 - accuracy: 0.3660\n",
            "Epoch 53/100\n",
            "927/927 [==============================] - 40s 43ms/step - loss: 2.7849 - accuracy: 0.3714\n",
            "Epoch 54/100\n",
            "927/927 [==============================] - 41s 44ms/step - loss: 2.7550 - accuracy: 0.3754\n",
            "Epoch 55/100\n",
            "927/927 [==============================] - 40s 44ms/step - loss: 2.7260 - accuracy: 0.3807\n",
            "Epoch 56/100\n",
            "927/927 [==============================] - 40s 43ms/step - loss: 2.6974 - accuracy: 0.3849\n",
            "Epoch 57/100\n",
            "927/927 [==============================] - 40s 43ms/step - loss: 2.6679 - accuracy: 0.3896\n",
            "Epoch 58/100\n",
            "927/927 [==============================] - 40s 43ms/step - loss: 2.6404 - accuracy: 0.3957\n",
            "Epoch 59/100\n",
            "927/927 [==============================] - 40s 43ms/step - loss: 2.6126 - accuracy: 0.3996\n",
            "Epoch 60/100\n",
            "927/927 [==============================] - 41s 44ms/step - loss: 2.5858 - accuracy: 0.4055\n",
            "Epoch 61/100\n",
            "927/927 [==============================] - 42s 45ms/step - loss: 2.5599 - accuracy: 0.4095\n",
            "Epoch 62/100\n",
            "927/927 [==============================] - 42s 45ms/step - loss: 2.5344 - accuracy: 0.4138\n",
            "Epoch 63/100\n",
            "927/927 [==============================] - 42s 45ms/step - loss: 2.5084 - accuracy: 0.4193\n",
            "Epoch 64/100\n",
            "927/927 [==============================] - 42s 45ms/step - loss: 2.4825 - accuracy: 0.4241\n",
            "Epoch 65/100\n",
            "927/927 [==============================] - 42s 45ms/step - loss: 2.4576 - accuracy: 0.4296\n",
            "Epoch 66/100\n",
            "927/927 [==============================] - 42s 45ms/step - loss: 2.4344 - accuracy: 0.4335\n",
            "Epoch 67/100\n",
            "927/927 [==============================] - 41s 45ms/step - loss: 2.4107 - accuracy: 0.4374\n",
            "Epoch 68/100\n",
            "927/927 [==============================] - 41s 44ms/step - loss: 2.3861 - accuracy: 0.4424\n",
            "Epoch 69/100\n",
            "927/927 [==============================] - 41s 44ms/step - loss: 2.3617 - accuracy: 0.4461\n",
            "Epoch 70/100\n",
            "927/927 [==============================] - 41s 45ms/step - loss: 2.3389 - accuracy: 0.4511\n",
            "Epoch 71/100\n",
            "927/927 [==============================] - 41s 45ms/step - loss: 2.3203 - accuracy: 0.4548\n",
            "Epoch 72/100\n",
            "927/927 [==============================] - 41s 45ms/step - loss: 2.2947 - accuracy: 0.4589\n",
            "Epoch 73/100\n",
            "927/927 [==============================] - 41s 45ms/step - loss: 2.2713 - accuracy: 0.4636\n",
            "Epoch 74/100\n",
            "927/927 [==============================] - 41s 45ms/step - loss: 2.2521 - accuracy: 0.4675\n",
            "Epoch 75/100\n",
            "927/927 [==============================] - 42s 45ms/step - loss: 2.2278 - accuracy: 0.4718\n",
            "Epoch 76/100\n",
            "927/927 [==============================] - 42s 45ms/step - loss: 2.2090 - accuracy: 0.4756\n",
            "Epoch 77/100\n",
            "927/927 [==============================] - 42s 45ms/step - loss: 2.1881 - accuracy: 0.4810\n",
            "Epoch 78/100\n",
            "927/927 [==============================] - 42s 45ms/step - loss: 2.1673 - accuracy: 0.4831\n",
            "Epoch 79/100\n",
            "927/927 [==============================] - 42s 45ms/step - loss: 2.1473 - accuracy: 0.4873\n",
            "Epoch 80/100\n",
            "927/927 [==============================] - 42s 45ms/step - loss: 2.1257 - accuracy: 0.4903\n",
            "Epoch 81/100\n",
            "927/927 [==============================] - 42s 45ms/step - loss: 2.1094 - accuracy: 0.4942\n",
            "Epoch 82/100\n",
            "927/927 [==============================] - 42s 45ms/step - loss: 2.0880 - accuracy: 0.4990\n",
            "Epoch 83/100\n",
            "927/927 [==============================] - 42s 45ms/step - loss: 2.0679 - accuracy: 0.5024\n",
            "Epoch 84/100\n",
            "927/927 [==============================] - 42s 45ms/step - loss: 2.0517 - accuracy: 0.5066\n",
            "Epoch 85/100\n",
            "927/927 [==============================] - 42s 45ms/step - loss: 2.0323 - accuracy: 0.5090\n",
            "Epoch 86/100\n",
            "927/927 [==============================] - 42s 45ms/step - loss: 2.0134 - accuracy: 0.5139\n",
            "Epoch 87/100\n",
            "927/927 [==============================] - 42s 45ms/step - loss: 1.9965 - accuracy: 0.5168\n",
            "Epoch 88/100\n",
            "927/927 [==============================] - 41s 45ms/step - loss: 1.9770 - accuracy: 0.5216\n",
            "Epoch 89/100\n",
            "927/927 [==============================] - 42s 45ms/step - loss: 1.9596 - accuracy: 0.5248\n",
            "Epoch 90/100\n",
            "927/927 [==============================] - 41s 45ms/step - loss: 1.9427 - accuracy: 0.5294\n",
            "Epoch 91/100\n",
            "927/927 [==============================] - 42s 45ms/step - loss: 1.9308 - accuracy: 0.5302\n",
            "Epoch 92/100\n",
            "927/927 [==============================] - 42s 45ms/step - loss: 1.9104 - accuracy: 0.5352\n",
            "Epoch 93/100\n",
            "927/927 [==============================] - 42s 45ms/step - loss: 1.8929 - accuracy: 0.5370\n",
            "Epoch 94/100\n",
            "927/927 [==============================] - 42s 45ms/step - loss: 1.8794 - accuracy: 0.5404\n",
            "Epoch 95/100\n",
            "927/927 [==============================] - 42s 45ms/step - loss: 1.8643 - accuracy: 0.5435\n",
            "Epoch 96/100\n",
            "927/927 [==============================] - 42s 45ms/step - loss: 1.8445 - accuracy: 0.5484\n",
            "Epoch 97/100\n",
            "927/927 [==============================] - 42s 45ms/step - loss: 1.8324 - accuracy: 0.5504\n",
            "Epoch 98/100\n",
            "927/927 [==============================] - 42s 45ms/step - loss: 1.8200 - accuracy: 0.5537\n",
            "Epoch 99/100\n",
            "927/927 [==============================] - 42s 45ms/step - loss: 1.8092 - accuracy: 0.5557\n",
            "Epoch 100/100\n",
            "927/927 [==============================] - 42s 45ms/step - loss: 1.7865 - accuracy: 0.5601\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f85141d0f90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAS7VZse3FIC"
      },
      "source": [
        "<a id=section605></a>\n",
        "### e. Save the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F85vCF63PGgo"
      },
      "source": [
        "- Here, we use the **Keras model API** to save the model to the file **‘model.h5‘** in the current working directory.\n",
        "\n",
        "- Later, when we **load the model** to make predictions.\n",
        "- We will also need the **mapping of words** to **integers**. \n",
        "- This is in the **Tokenizer object**, and we can save that too **using Pickle**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TG1hkckPMQg"
      },
      "source": [
        "# save the model to file\n",
        "model.save('model.h5')\n",
        "# save the tokenizer\n",
        "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTVXdfzw3FIN"
      },
      "source": [
        "<a id=section7></a>\n",
        "## 7. Use Language model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIv8u9Tz3FIP"
      },
      "source": [
        "- In this case, we can use the model to generate **new sequences of text** that have the same **statistical properties** as the source text.\n",
        "\n",
        "- We will start by **loading** the **training sequences** again.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmLOxH41PQlx"
      },
      "source": [
        "<a id=section701></a>\n",
        "### a. Load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwtQz4N3PZ-1"
      },
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, 'r')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "# load cleaned text sequences\n",
        "in_filename = 'republic_sequences.txt'\n",
        "doc = load_doc(in_filename)\n",
        "lines = doc.split('\\n')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDiucNmFPd02"
      },
      "source": [
        "[](http://)\n",
        "\n",
        "- We need the text so that we can choose a **source sequence** as input to the model for generating a **new sequence of text**.\n",
        "\n",
        "- The model will require **50 words** as **input**.\n",
        "\n",
        "- Later, we will need to specify the **expected length of input**.\n",
        "- We can determine this from the **input sequences** by **calculating the length** of one line of the loaded data and **subtracting** **1** for the **expected output** word that is also on the same line.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_uNuSY7PhVP"
      },
      "source": [
        "seq_length = len(lines[0].split()) - 1"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUEry7f1PjtJ"
      },
      "source": [
        "<a id=section702></a>\n",
        "### b. Load Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3G2SzwJ3FId"
      },
      "source": [
        "- We can now **load the model** from file.\n",
        "\n",
        "\n",
        "- Keras provides the **load_model() function** for loading the model, ready for use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAIRVWtaPp2l"
      },
      "source": [
        "from random import randint\n",
        "from pickle import load\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# load the model\n",
        "model = load_model('model.h5')\n",
        "\n",
        "# load the tokenizer\n",
        "tokenizer = load(open('tokenizer.pkl', 'rb'))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjdIpsuK3FIg"
      },
      "source": [
        "<a id=section703></a>\n",
        "### c. Generate text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKnsr61-Puwe"
      },
      "source": [
        "* The first step in generating text is **preparing a seed input**.\n",
        "\n",
        "\n",
        "* We will select a **random line** of text from the **input text** for this purpose. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvyF0mPZHcy2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ed1dfa7-c2cf-471e-9b95-8049c01a0d2b"
      },
      "source": [
        "from random import randint\n",
        "from pickle import load\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "# generate a sequence from a language model\n",
        "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
        "    result = list()\n",
        "    in_text = seed_text\n",
        "    # generate a fixed number of words\n",
        "    for _ in range(n_words):\n",
        "        # encode the text as integer\n",
        "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        # truncate sequences to a fixed length\n",
        "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "        # predict probabilities for each word\n",
        "        yhat = model.predict_classes(encoded, verbose=0)\n",
        "        # map predicted word index to word\n",
        "        out_word = ''\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == yhat:\n",
        "                out_word = word\n",
        "                break\n",
        "        # append to input\n",
        "        in_text += ' ' + out_word\n",
        "        result.append(out_word)\n",
        "    return ' '.join(result)\n",
        "\n",
        "\n",
        "# load the model\n",
        "model = load_model('model.h5')\n",
        "\n",
        "# load the tokenizer\n",
        "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
        "\n",
        "# select a seed text\n",
        "seed_text = lines[randint(0,len(lines))]\n",
        "print(\"seed_text:\" + '\\n')\n",
        "print(seed_text + '\\n')\n",
        "\n",
        "# generate new text\n",
        "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
        "print(\"generated_text:\" + '\\n')\n",
        "print(generated)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "seed_text:\n",
            "\n",
            "live well and the unjust man will live ill that is what your argument proves and he who lives well is blessed and happy and he who lives ill the reverse of happy certainly then the just is happy and the unjust miserable so be it but happiness and not misery\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "generated_text:\n",
            "\n",
            "is profitable for either or if there were no war in relation to mind thrasymachus and could he not be sophisms find a mans own light or ridiculous causes she gives the former times into the truer matters of the unjust having fallen up the chaos was taught him and\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3SvYiMo3FIm"
      },
      "source": [
        " - We can see that the text seems reasonable. In fact, the addition of concatenation would help in interpreting the seed and the generated text. Nevertheless, the generated text gets the right kind of words in the right kind of order.\n",
        "\n"
      ]
    }
  ]
}