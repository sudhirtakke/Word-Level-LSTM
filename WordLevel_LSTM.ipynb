{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "WordLevel LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sudhirtakke/Word-Level-LSTM/blob/main/WordLevel_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBO78ScK3FFx"
      },
      "source": [
        "# Building new dialogues using Keras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dOj1B8d3FFz"
      },
      "source": [
        "### Table of Contents\n",
        "\n",
        "1. [Learning Goals](#section1)\n",
        "2. [Language Model Design](#section2)\n",
        "3. [Load Text](#section3)\n",
        "4. [Clean Text](#section4)\n",
        "5. [Save Cleaned Text](#section5)\n",
        "6. [Train Language Model](#section6)\n",
        " - a. [Load Sequences](#section601)\n",
        " - b. [Encode Sequences](#section602)\n",
        " - c. [Sequence Inputs and Output](#section603)\n",
        " - d. [Fit Model](#section604)\n",
        " - e. [Save the model](#section605)\n",
        "7. [Use Language model](#section7)\n",
        " - a. [Load the Data](#section701)\n",
        " - b. [Load Model](#section702)\n",
        " - c. [Generate Text](#section703)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-oYCORA4oCf",
        "toc-hr-collapsed": false
      },
      "source": [
        "\n",
        "<br>\n",
        "\n",
        "* We are going to develop **word-level neural language model** and use it to generate text.\n",
        "\n",
        "* A **language model** can predict the probability of the next word in the sequence, based on the **words already observed** in the sequence.\n",
        "\n",
        "* **Neural network models** are a preferred method for **developing statistical language models** because they can use a **distributed representation** where different words with similar meanings have **similar representation**.\n",
        "\n",
        "- Also, it is because they can use a **large context** of recently observed words when **making predictions**.\n",
        "\n",
        "\n",
        "\n",
        "<a id=section1></a>\n",
        "## 1. Learning goals\n",
        " \n",
        "\n",
        "* How to prepare text for developing a **word-based language** model ?\n",
        "* How to design and fit a **neural language model** with a **learned embedding** and an **LSTM hidden layer** ?\n",
        "* How to use the **learned language model** to generate **new text** with **similar statistical properties** as the source text ?\n",
        "\n",
        "### Overview\n",
        "1. The Republic by Plato\n",
        "2. Data Preparation\n",
        "3. Train Language Model\n",
        "4. Use Language Model\n",
        "\n",
        "---\n",
        "\n",
        "## The Republic by Plato\n",
        "<br>\n",
        "\n",
        "- Download the ASCII **text version** of the entire book (or books) here: [The Republic](https://https://www.gutenberg.org/ebooks/1497) and save it as *republic.txt*\n",
        "\n",
        "- **Open the file in a text editor and delete the front and back matter. This includes details about the book at the beginning, a long analysis, and license information at the end.**\n",
        "\n",
        "## Data Preparation\n",
        "\n",
        "- We will start by **preparing the data** for modeling.\n",
        "\n",
        "- The first step is to look at the data.\n",
        "\n",
        "### Review the Text\n",
        "- Open the text in an editor and just look at the text data.\n",
        "\n",
        "- For example, here is the first piece of dialog:\n",
        "\n",
        "> BOOK I.\n",
        "\n",
        "        I went down yesterday to the Piraeus with Glaucon the son of Ariston,\n",
        "        that I might offer up my prayers to the goddess (Bendis, the Thracian\n",
        "        Artemis.); and also because I wanted to see in what manner they would\n",
        "        celebrate the festival, which was a new thing. I was delighted with the\n",
        "        procession of the inhabitants; but that of the Thracians was equally,\n",
        "        if not more, beautiful. When we had finished our prayers and viewed the\n",
        "        spectacle, we turned in the direction of the city; and at that instant\n",
        "        Polemarchus the son of Cephalus chanced to catch sight of us from a\n",
        "        distance as we were starting on our way home, and told his servant to\n",
        "        run and bid us wait for him. The servant took hold of me by the cloak\n",
        "        behind, and said: Polemarchus desires you to wait.\n",
        "\n",
        "        I turned round, and asked him where his master was.\n",
        "\n",
        "        There he is, said the youth, coming after you, if you will only wait.\n",
        "\n",
        "        Certainly we will, said Glaucon; and in a few minutes Polemarchus\n",
        "        appeared, and with him Adeimantus, Glaucon’s brother, Niceratus the son\n",
        "        of Nicias, and several others who had been at the procession.\n",
        "\n",
        "        Polemarchus said to me: I perceive, Socrates, that you and your\n",
        "        companion are already on your way to the city.\n",
        "\n",
        "        You are not far wrong, I said.\n",
        "        ...\n",
        "        \n",
        "### Here’s what we see from a quick look:\n",
        "\n",
        "* Book/Chapter headings (e.g. “BOOK I.”).\n",
        "* British English spelling (e.g. “honoured”)\n",
        "* Lots of punctuation (e.g. “–“, “;–“, “?–“, and more)\n",
        "* Strange names (e.g. “Polemarchus”).\n",
        "* Some long monologues that go on for hundreds of lines.\n",
        "* Some quoted dialog (e.g. ‘…’)\n",
        "\n",
        "\n",
        "<a id=section2></a>\n",
        "### 2. PLAN:  Language model design\n",
        "\n",
        "- It will be **statistical** and will **predict the probability** of **each word** given an input sequence of text.\n",
        "- The **predicted word** will be fed in as input to in turn **generate the next word**.\n",
        "\n",
        "- A key design decision is how long the **input sequences** should be.\n",
        "- They need to be long enough to allow the model to **learn the context** for the words to predict.\n",
        "- This **input length** will also define the **length of seed text** used to generate **new sequences** when we use the model.\n",
        "- There is **no correct** answer.\n",
        "- With enough time and resources, we could explore the **ability of the model** to learn with **differently sized input sequences**.\n",
        "\n",
        "- Instead, we will pick a length of **50 words** for the length of the **input sequences**, somewhat arbitrarily."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvysmaT93FF6"
      },
      "source": [
        "We'll be using the following **process sequence** in this notebook:\n",
        "![](https://raw.githubusercontent.com/insaid2018/DeepLearning/master/images/word_lstm_flow0.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06Z_9cCHE8-u"
      },
      "source": [
        "<a id=section3></a>\n",
        "### 3. Load Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Np8phFLO3FGA"
      },
      "source": [
        "- The first step is to **load the text** into **memory** as a **sequence** of **loaded text**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItkIWd-QuyJn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b56cdedf-e2a7-42a4-f80b-bbcfd4143a33"
      },
      "source": [
        "# Import tensorflow 2.x\n",
        "# This code block will only work in Google Colab.\n",
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gq0Ve7ItvsQP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "59c1e28e-ac86-40a2-b508-4967dd66c5dd"
      },
      "source": [
        "import urllib\n",
        "response = urllib.request.urlopen('https://raw.githubusercontent.com/insaid2018/DeepLearning/master/Data/republic_clean.txt')\n",
        "doc = response.read().decode('utf8')\n",
        "print(doc[:200])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "﻿\rBOOK I.\r\r\n",
            "\r\r\n",
            "I went down yesterday to the Piraeus with Glaucon the son of Ariston,\r\r\n",
            "that I might offer up my prayers to the goddess (Bendis, the Thracian\r\r\n",
            "Artemis.); and also because I wanted to s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkV_4Y0SGNL1"
      },
      "source": [
        "<a id=section4></a>\n",
        "### 4. Clean Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKSbwDDO3FGU"
      },
      "source": [
        "- We need to transform the **raw text** into a **sequence of tokens** or words that we can use as a source to train the model.\n",
        "\n",
        "- Based on **reviewing the raw text** (above), below are some specific operations we will perform to clean the text. \n",
        "- We may want to explore **more cleaning operations** as an extension.\n",
        "\n",
        "* **Replace ‘–‘** with a white space so we can split words better.\n",
        "* **Split words** based on **white space**.\n",
        "* Remove all **punctuation** from **words** to reduce the vocabulary size (e.g. ‘What?’ becomes ‘What’).\n",
        "* **Remove all words** that are not alphabetic to remove standalone **punctuation tokens**.\n",
        "* Normalize **all words** to **lowercase** to reduce the **vocabulary size**.\n",
        "\n",
        "\n",
        "- **Vocabulary size** is a big deal with language modeling.\n",
        "- A **smaller vocabulary** results in a **smaller model** that **trains faster.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCORHQHPGmTC"
      },
      "source": [
        "import string\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "    # replace '--' with a space ' '\n",
        "    doc = doc.replace('--', ' ')\n",
        "    # split into tokens by white space\n",
        "    tokens = doc.split()\n",
        "    # remove punctuation from each token\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    tokens = [w.translate(table) for w in tokens]\n",
        "    # remove remaining tokens that are not alphabetic\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    # make lower case\n",
        "    tokens = [word.lower() for word in tokens]\n",
        "    return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPb4XvVCGslQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "e7aef252-3492-4d45-f742-49d0bed38225"
      },
      "source": [
        "# clean document\n",
        "tokens = clean_doc(doc)\n",
        "print(tokens[:200])\n",
        "print('Total Tokens: %d' % len(tokens))\n",
        "print('Unique Tokens: %d' % len(set(tokens)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['book', 'i', 'i', 'went', 'down', 'yesterday', 'to', 'the', 'piraeus', 'with', 'glaucon', 'the', 'son', 'of', 'ariston', 'that', 'i', 'might', 'offer', 'up', 'my', 'prayers', 'to', 'the', 'goddess', 'bendis', 'the', 'thracian', 'artemis', 'and', 'also', 'because', 'i', 'wanted', 'to', 'see', 'in', 'what', 'manner', 'they', 'would', 'celebrate', 'the', 'festival', 'which', 'was', 'a', 'new', 'thing', 'i', 'was', 'delighted', 'with', 'the', 'procession', 'of', 'the', 'inhabitants', 'but', 'that', 'of', 'the', 'thracians', 'was', 'equally', 'if', 'not', 'more', 'beautiful', 'when', 'we', 'had', 'finished', 'our', 'prayers', 'and', 'viewed', 'the', 'spectacle', 'we', 'turned', 'in', 'the', 'direction', 'of', 'the', 'city', 'and', 'at', 'that', 'instant', 'polemarchus', 'the', 'son', 'of', 'cephalus', 'chanced', 'to', 'catch', 'sight', 'of', 'us', 'from', 'a', 'distance', 'as', 'we', 'were', 'starting', 'on', 'our', 'way', 'home', 'and', 'told', 'his', 'servant', 'to', 'run', 'and', 'bid', 'us', 'wait', 'for', 'him', 'the', 'servant', 'took', 'hold', 'of', 'me', 'by', 'the', 'cloak', 'behind', 'and', 'said', 'polemarchus', 'desires', 'you', 'to', 'wait', 'i', 'turned', 'round', 'and', 'asked', 'him', 'where', 'his', 'master', 'was', 'there', 'he', 'is', 'said', 'the', 'youth', 'coming', 'after', 'you', 'if', 'you', 'will', 'only', 'wait', 'certainly', 'we', 'will', 'said', 'glaucon', 'and', 'in', 'a', 'few', 'minutes', 'polemarchus', 'appeared', 'and', 'with', 'him', 'adeimantus', 'glaucons', 'brother', 'niceratus', 'the', 'son', 'of', 'nicias', 'and', 'several', 'others', 'who', 'had', 'been', 'at', 'the', 'procession', 'polemarchus', 'said']\n",
            "Total Tokens: 118684\n",
            "Unique Tokens: 7409\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9bm-2ZyGy-q"
      },
      "source": [
        "<a id=section5></a>\n",
        "### 5. Save clean text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xf7dLooF3FGp"
      },
      "source": [
        "- We can organize the long list of tokens into sequences of **50 input words** and **1 output word**.\n",
        "\n",
        "- That is, sequences of **51 words**.\n",
        "\n",
        "- We can do this by iterating over the list of tokens from token 51 onwards and taking the prior **50 tokens as a sequence**, then repeating this process to the end of the list of tokens.\n",
        "\n",
        "- We will transform the tokens into **space-separated strings** for later storage in a file.\n",
        "\n",
        "- The code to split the list of **clean tokens** into **sequences with a length of 51 tokens** is listed below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOI6sfwaG8QB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "43371476-ce20-4d75-d830-ddfb8b2bc63d"
      },
      "source": [
        "# organize into sequences of tokens\n",
        "length = 50 + 1\n",
        "sequences = list()\n",
        "for i in range(length, len(tokens)):\n",
        "    # select sequence of tokens\n",
        "    seq = tokens[i-length:i]\n",
        "    # convert into a line\n",
        "    line = ' '.join(seq)\n",
        "    # store\n",
        "    sequences.append(line)\n",
        "print('Total Sequences: %d' % len(sequences))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Sequences: 118633\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5YCAlga3FGw",
        "outputId": "eaa56ffa-ad01-4c3e-f4e4-e960907a8219"
      },
      "source": [
        "sequences[:2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['book i i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was',\n",
              " 'i i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RksKge0_G_Oj"
      },
      "source": [
        "# save tokens to file, one dialog per line\n",
        "def save_doc(lines, filename):\n",
        "    data = '\\n'.join(lines)\n",
        "    file = open(filename, 'w')\n",
        "    file.write(data)\n",
        "    file.close()\n",
        "    \n",
        "# save sequences to file\n",
        "out_filename = 'republic_sequences.txt'\n",
        "save_doc(sequences, out_filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IwlwV4_H3OW"
      },
      "source": [
        "<a id=section6></a>\n",
        "## 6. Train Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHCFMESq3FG7"
      },
      "source": [
        "We can now train a **statistical language mode**l from the prepared data.\n",
        "\n",
        "The model we will train is a neural language model. It has a few unique characteristics:\n",
        "\n",
        "* It uses a **distributed representation for words** so that different words with similar meanings will have a similar representation.\n",
        "* It **learns** the **representation** at the same time as **learning the model.**\n",
        "* It **learns** to **predict the probability** for the next word using the context of the last 100 words.\n",
        "\n",
        "Specifically, we will use an **Embedding Layer** to learn the representation of words, and a **Long Short-Term Memory (LSTM)** recurrent neural network to learn to **predict words** based on their context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rL1mwuBc3FG-"
      },
      "source": [
        "### Let’s start by loading our training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NmWD7bb3FG_"
      },
      "source": [
        "<a id=section601></a>\n",
        "### a. Load Sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBgoos1p3FHB"
      },
      "source": [
        "- We can load our **training data** using the **`load_doc()`** function defined below.\n",
        "\n",
        "\n",
        "- Once loaded, we can **split the data into separate training sequences** by splitting based on new lines.\n",
        "\n",
        "\n",
        "- The snippet below will load the **‘republic_sequences.txt‘** data file from the current working directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYZIaBv-IRDJ"
      },
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, 'r')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "# load\n",
        "in_filename = 'republic_sequences.txt'\n",
        "doc = load_doc(in_filename)\n",
        "lines = doc.split('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7kz_wjL3FHH",
        "outputId": "d6489139-41c3-4360-bbd3-8e2cbaee2b3d"
      },
      "source": [
        "lines[:2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['book i i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was',\n",
              " 'i i went down yesterday to the piraeus with glaucon the son of ariston that i might offer up my prayers to the goddess bendis the thracian artemis and also because i wanted to see in what manner they would celebrate the festival which was a new thing i was delighted']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhFPp4lxIW1E"
      },
      "source": [
        "<a id=section602></a>\n",
        "### b. Encode Sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdcRYdZI3FHN"
      },
      "source": [
        "- The **word embedding layer** expects input sequences to be comprised of integers.\n",
        "\n",
        "- We can **map each word in our vocabulary** to a unique integer and encode our input sequences.\n",
        "- Later, when we make predictions, we can convert the **prediction to numbers** and look up their **associated words** in the **same mapping**.\n",
        "\n",
        "- To do this **encoding**, we will use the **`Tokenizer`** class in the Keras API.\n",
        "\n",
        "- First, the **Tokenizer** must be trained on the **entire training dataset**, which means it finds all of the unique words in the data and assigns each a unique integer.\n",
        "\n",
        "- We can then use the **fit Tokenizer** to encode all of the training sequences, **converting each sequence** from a **list of words** to a **list of integers**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDn6Si02IeI6"
      },
      "source": [
        "from numpy import array\n",
        "from pickle import dump\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "# integer encode sequences of words\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(lines)\n",
        "sequences = tokenizer.texts_to_sequences(lines)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxR_DnR9I7Zt"
      },
      "source": [
        "- We can access the **mapping of words to integers** as a dictionary attribute called **word_index on the Tokenizer object**.\n",
        "\n",
        "- We need to know the **size of the vocabulary** for defining the embedding layer later. \n",
        "- We can determine the vocabulary by **calculating the size** of the **mapping dictionary**.\n",
        "\n",
        "- Words are assigned values from **1 to the total number of words** (e.g. 7,409).\n",
        "- The **Embedding layer** needs to allocate a **vector representation** for each word in this vocabulary from **index 1 to the largest index** \n",
        "- It is because indexing of arrays is **zero-offset**, the index of the word at the end of the vocabulary will be 7,409\n",
        "- This means the array must be **7,409 + 1** in length.\n",
        "\n",
        "- Therefore, when specifying the **vocabulary size** to the **Embedding layer**, we specify it as 1 larger than the **actual vocabulary**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2ZZJ3DxI_2-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d8f5d381-b493-4720-a135-a5de64c31dbe"
      },
      "source": [
        "# vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "vocab_size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7410"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWagP5ehJCGL"
      },
      "source": [
        "<a id=section603></a>\n",
        "### c. Sequence Inputs and Output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0lorZEM3FHZ"
      },
      "source": [
        "- Now that we have encoded the input sequences, we need to separate them into input (X) and output (y) elements.\n",
        "\n",
        "- We can do this with **array slicing**.\n",
        "\n",
        "- After separating, we need to **one hot encode the output word**. \n",
        "- This means converting it from an integer to a vector of 0 values, one for each word in the vocabulary, with a **1 to indicate the specific word** at the index of the words integer value.\n",
        "\n",
        "- This is so that the model learns to **predict the probability distribution** for the next word and the ground truth from which to learn from is 0 for all words except the actual word that comes next.\n",
        "\n",
        "- Keras provides the **to_categorical()** that can be used to **one hot encode** the output words for each **input-output sequence** pair.\n",
        "\n",
        "- Finally, we need to specify to the **Embedding layer** how long input sequences are. \n",
        "- We know that there are **50 words** because we designed the model, but a good generic way to specify that is to use the **second dimension (number of columns)** of the input data’s shape. \n",
        "- That way, if We change the **length of sequences** when preparing data, We do not need to change this **data loading code**; it is generic."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PACdSAPYJAbT",
        "scrolled": true
      },
      "source": [
        "# separate into input and output\n",
        "sequences = array(sequences)\n",
        "X, y = sequences[:,:-1], sequences[:,-1]\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "seq_length = X.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugQoKt9G3FHe",
        "outputId": "d005d944-2860-4b28-c0be-f239cba81511"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(118633, 50)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WbqBTEc3FHm",
        "outputId": "cf80d971-630f-4d4c-ca03-cfd0b3ca445e"
      },
      "source": [
        "X[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1046,   11,   11, 1045,  329, 7409,    4,    1, 2873,   35,  213,\n",
              "          1,  261,    3, 2251,    9,   11,  179,  817,  123,   92, 2872,\n",
              "          4,    1, 2249, 7408,    1, 7407, 7406,    2,   75,  120,   11,\n",
              "       1266,    4,  110,    6,   30,  168,   16,   49, 7405,    1, 1609,\n",
              "         13,   57,    8,  549,  151,   11])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "easmWXJu3FHv",
        "outputId": "22decdfb-f907-46df-a147-66b673552c44"
      },
      "source": [
        "y.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(118633, 7410)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9G_pEZAf3FHy",
        "outputId": "6690fd76-f309-4fe0-8445-78ff708cc85b"
      },
      "source": [
        "y[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9PvtEQA3FH0"
      },
      "source": [
        "<a id=section604></a>\n",
        "### d. Fit Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zw5tiv7JKwX"
      },
      "source": [
        "- We can now **define and fit** our language model on the training data.\n",
        "\n",
        "- The **learned embedding** needs to know the size of the vocabulary and the **length of input sequences** as previously discussed.\n",
        "\n",
        " - **Size of the embedding vector space**: a parameter to specify how many dimensions will be used to represent each word\n",
        "\n",
        "- Common values are **50, 100, and 300**. \n",
        "- We will use 50 here, but consider **testing smaller or larger values**.\n",
        "\n",
        "- We will use a **two LSTM hidden layers** with **100 memory cells** each. \n",
        "- More **memory cells** and a **deeper network** may achieve better results.\n",
        "\n",
        " \n",
        "###  Procedure:\n",
        "\n",
        " - A **dense fully connected layer** with **100 neurons** connects to the **LSTM hidden layers** to interpret the features extracted from the sequence. \n",
        " - The **output layer** predicts the **next word** as a single vector the **size of the vocabulary** with a probability for each word in the vocabulary. \n",
        " - A **softmax activation functio**n is used to **ensure the outputs** have the characteristics of normalized probabilities.\n",
        " \n",
        " <center><img src = \"https://raw.githubusercontent.com/insaid2018/Term-1/master/Images/images.png\"/></center>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmzS7OUuJIps",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        },
        "outputId": "3eb76106-825a-4d9c-b7c2-2900d47fb7bf"
      },
      "source": [
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
        "model.add(LSTM(100, return_sequences=True))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 50, 50)            370500    \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 50, 100)           60400     \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 100)               80400     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 7410)              748410    \n",
            "=================================================================\n",
            "Total params: 1,269,810\n",
            "Trainable params: 1,269,810\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-2JH1NlJYcW"
      },
      "source": [
        "-  The model is compiled specifying the **categorical cross entropy loss** needed to fit the model.\n",
        "- Technically, the **model** is learning a **multi-class classification** and this is the suitable loss function for this type of problem. \n",
        "- The efficient **Adam implementation** to **mini-batch gradient descent** is used and accuracy is evaluated of the model.\n",
        "\n",
        "- Finally, the **model** is fit on the data for **100 training epochs** with a modest batch size of 128 to speed things up.\n",
        "\n",
        "- Training may take a **few hours on modern hardware** without GPUs. \n",
        "- We can speed it up with a **larger batch size** and/or fewer training epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrgQUag_JUwW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "outputId": "dc9645da-01b2-4484-d34f-45b235a7a6e6"
      },
      "source": [
        "# compile model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit model\n",
        "model.fit(X, y, batch_size=128, epochs=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "118633/118633 [==============================] - 138s 1ms/step - loss: 6.1902 - acc: 0.0666\n",
            "Epoch 2/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 5.7708 - acc: 0.0983\n",
            "Epoch 3/100\n",
            "118633/118633 [==============================] - 134s 1ms/step - loss: 5.5591 - acc: 0.1187\n",
            "Epoch 4/100\n",
            "118633/118633 [==============================] - 134s 1ms/step - loss: 5.4088 - acc: 0.1306\n",
            "Epoch 5/100\n",
            "118633/118633 [==============================] - 134s 1ms/step - loss: 5.2885 - acc: 0.1421\n",
            "Epoch 6/100\n",
            "118633/118633 [==============================] - 133s 1ms/step - loss: 5.1942 - acc: 0.1520\n",
            "Epoch 7/100\n",
            "118633/118633 [==============================] - 134s 1ms/step - loss: 5.1165 - acc: 0.1552\n",
            "Epoch 8/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 5.0469 - acc: 0.1596\n",
            "Epoch 9/100\n",
            "118633/118633 [==============================] - 134s 1ms/step - loss: 4.9811 - acc: 0.1620\n",
            "Epoch 10/100\n",
            "118633/118633 [==============================] - 134s 1ms/step - loss: 4.9142 - acc: 0.1677\n",
            "Epoch 11/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 4.8508 - acc: 0.1722\n",
            "Epoch 12/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 4.7896 - acc: 0.1772\n",
            "Epoch 13/100\n",
            "118633/118633 [==============================] - 134s 1ms/step - loss: 4.7338 - acc: 0.1816\n",
            "Epoch 14/100\n",
            "118633/118633 [==============================] - 134s 1ms/step - loss: 4.6806 - acc: 0.1848\n",
            "Epoch 15/100\n",
            "118633/118633 [==============================] - 134s 1ms/step - loss: 4.6422 - acc: 0.1866\n",
            "Epoch 16/100\n",
            "118633/118633 [==============================] - 133s 1ms/step - loss: 4.7516 - acc: 0.1778\n",
            "Epoch 17/100\n",
            "118633/118633 [==============================] - 132s 1ms/step - loss: 4.6509 - acc: 0.1839\n",
            "Epoch 18/100\n",
            "118633/118633 [==============================] - 130s 1ms/step - loss: 4.5721 - acc: 0.1915\n",
            "Epoch 19/100\n",
            "118633/118633 [==============================] - 130s 1ms/step - loss: 4.5173 - acc: 0.1951\n",
            "Epoch 20/100\n",
            "118633/118633 [==============================] - 129s 1ms/step - loss: 4.4630 - acc: 0.1984\n",
            "Epoch 21/100\n",
            "118633/118633 [==============================] - 129s 1ms/step - loss: 4.4135 - acc: 0.2020\n",
            "Epoch 22/100\n",
            "118633/118633 [==============================] - 130s 1ms/step - loss: 4.3711 - acc: 0.2035\n",
            "Epoch 23/100\n",
            "118633/118633 [==============================] - 129s 1ms/step - loss: 4.3291 - acc: 0.2065\n",
            "Epoch 24/100\n",
            "118633/118633 [==============================] - 130s 1ms/step - loss: 4.2663 - acc: 0.2108\n",
            "Epoch 25/100\n",
            "118633/118633 [==============================] - 133s 1ms/step - loss: 4.2134 - acc: 0.2140\n",
            "Epoch 26/100\n",
            "118633/118633 [==============================] - 134s 1ms/step - loss: 4.1783 - acc: 0.2154\n",
            "Epoch 27/100\n",
            "118633/118633 [==============================] - 133s 1ms/step - loss: 4.1498 - acc: 0.2175\n",
            "Epoch 28/100\n",
            "118633/118633 [==============================] - 134s 1ms/step - loss: 4.1175 - acc: 0.2194\n",
            "Epoch 29/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 4.1845 - acc: 0.2123\n",
            "Epoch 30/100\n",
            "118633/118633 [==============================] - 134s 1ms/step - loss: 4.1224 - acc: 0.2175\n",
            "Epoch 31/100\n",
            "118633/118633 [==============================] - 134s 1ms/step - loss: 4.0775 - acc: 0.2199\n",
            "Epoch 32/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 4.0386 - acc: 0.2238\n",
            "Epoch 33/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 3.9981 - acc: 0.2271\n",
            "Epoch 34/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 3.9668 - acc: 0.2291\n",
            "Epoch 35/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 3.9288 - acc: 0.2335\n",
            "Epoch 36/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 3.8928 - acc: 0.2350\n",
            "Epoch 37/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 3.8631 - acc: 0.2390\n",
            "Epoch 38/100\n",
            "118633/118633 [==============================] - 134s 1ms/step - loss: 3.8343 - acc: 0.2418\n",
            "Epoch 39/100\n",
            "118633/118633 [==============================] - 134s 1ms/step - loss: 3.8917 - acc: 0.2366\n",
            "Epoch 40/100\n",
            "118633/118633 [==============================] - 134s 1ms/step - loss: 3.8872 - acc: 0.2353\n",
            "Epoch 41/100\n",
            "118633/118633 [==============================] - 134s 1ms/step - loss: 3.8548 - acc: 0.2410\n",
            "Epoch 42/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 3.8132 - acc: 0.2437\n",
            "Epoch 43/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 3.7707 - acc: 0.2491\n",
            "Epoch 44/100\n",
            "118633/118633 [==============================] - 134s 1ms/step - loss: 3.7625 - acc: 0.2497\n",
            "Epoch 45/100\n",
            "118633/118633 [==============================] - 134s 1ms/step - loss: 3.7447 - acc: 0.2525\n",
            "Epoch 46/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 3.7106 - acc: 0.2544\n",
            "Epoch 47/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 3.6790 - acc: 0.2575\n",
            "Epoch 48/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 3.7335 - acc: 0.2518\n",
            "Epoch 49/100\n",
            "118633/118633 [==============================] - 136s 1ms/step - loss: 3.6997 - acc: 0.2569\n",
            "Epoch 50/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 3.6680 - acc: 0.2609\n",
            "Epoch 51/100\n",
            "118633/118633 [==============================] - 138s 1ms/step - loss: 3.6329 - acc: 0.2647\n",
            "Epoch 52/100\n",
            "118633/118633 [==============================] - 138s 1ms/step - loss: 3.6061 - acc: 0.2668\n",
            "Epoch 53/100\n",
            "118633/118633 [==============================] - 139s 1ms/step - loss: 3.5709 - acc: 0.2716\n",
            "Epoch 54/100\n",
            "118633/118633 [==============================] - 137s 1ms/step - loss: 3.5669 - acc: 0.2720\n",
            "Epoch 55/100\n",
            "118633/118633 [==============================] - 138s 1ms/step - loss: 3.5531 - acc: 0.2743\n",
            "Epoch 56/100\n",
            "118633/118633 [==============================] - 134s 1ms/step - loss: 3.5136 - acc: 0.2777\n",
            "Epoch 57/100\n",
            "118633/118633 [==============================] - 133s 1ms/step - loss: 3.4769 - acc: 0.2824\n",
            "Epoch 58/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 3.4474 - acc: 0.2864\n",
            "Epoch 59/100\n",
            "118633/118633 [==============================] - 134s 1ms/step - loss: 3.4185 - acc: 0.2910\n",
            "Epoch 60/100\n",
            "118633/118633 [==============================] - 134s 1ms/step - loss: 3.3894 - acc: 0.2948\n",
            "Epoch 61/100\n",
            "118633/118633 [==============================] - 134s 1ms/step - loss: 3.3698 - acc: 0.2966\n",
            "Epoch 62/100\n",
            "118633/118633 [==============================] - 136s 1ms/step - loss: 3.3471 - acc: 0.3001\n",
            "Epoch 63/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 3.3651 - acc: 0.2989\n",
            "Epoch 64/100\n",
            "118633/118633 [==============================] - 136s 1ms/step - loss: 3.4428 - acc: 0.2901\n",
            "Epoch 65/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 3.3848 - acc: 0.2963\n",
            "Epoch 66/100\n",
            "118633/118633 [==============================] - 134s 1ms/step - loss: 3.3212 - acc: 0.3057\n",
            "Epoch 67/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 3.2815 - acc: 0.3092\n",
            "Epoch 68/100\n",
            "118633/118633 [==============================] - 134s 1ms/step - loss: 3.2498 - acc: 0.3147\n",
            "Epoch 69/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 3.2380 - acc: 0.3149\n",
            "Epoch 70/100\n",
            "118633/118633 [==============================] - 134s 1ms/step - loss: 3.1985 - acc: 0.3230\n",
            "Epoch 71/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 3.2265 - acc: 0.3186\n",
            "Epoch 72/100\n",
            "118633/118633 [==============================] - 134s 1ms/step - loss: 3.1872 - acc: 0.3230\n",
            "Epoch 73/100\n",
            "118633/118633 [==============================] - 134s 1ms/step - loss: 3.1611 - acc: 0.3269\n",
            "Epoch 74/100\n",
            "118633/118633 [==============================] - 134s 1ms/step - loss: 3.1219 - acc: 0.3326\n",
            "Epoch 75/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 3.0906 - acc: 0.3379\n",
            "Epoch 76/100\n",
            "118633/118633 [==============================] - 137s 1ms/step - loss: 3.0649 - acc: 0.3422\n",
            "Epoch 77/100\n",
            "118633/118633 [==============================] - 139s 1ms/step - loss: 3.0385 - acc: 0.3442\n",
            "Epoch 78/100\n",
            "118633/118633 [==============================] - 140s 1ms/step - loss: 3.0094 - acc: 0.3496\n",
            "Epoch 79/100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "118633/118633 [==============================] - 138s 1ms/step - loss: 2.9889 - acc: 0.3539\n",
            "Epoch 80/100\n",
            "118633/118633 [==============================] - 138s 1ms/step - loss: 2.9617 - acc: 0.3571\n",
            "Epoch 81/100\n",
            "118633/118633 [==============================] - 136s 1ms/step - loss: 2.9625 - acc: 0.3575\n",
            "Epoch 82/100\n",
            "118633/118633 [==============================] - 136s 1ms/step - loss: 2.9518 - acc: 0.3605\n",
            "Epoch 83/100\n",
            "118633/118633 [==============================] - 136s 1ms/step - loss: 2.9235 - acc: 0.3651\n",
            "Epoch 84/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 2.9103 - acc: 0.3675\n",
            "Epoch 85/100\n",
            "118633/118633 [==============================] - 136s 1ms/step - loss: 2.9091 - acc: 0.3675\n",
            "Epoch 86/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 2.9021 - acc: 0.3698\n",
            "Epoch 87/100\n",
            "118633/118633 [==============================] - 136s 1ms/step - loss: 2.8361 - acc: 0.3787\n",
            "Epoch 88/100\n",
            "118633/118633 [==============================] - 134s 1ms/step - loss: 2.8087 - acc: 0.3838\n",
            "Epoch 89/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 2.7992 - acc: 0.3850\n",
            "Epoch 90/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 2.8173 - acc: 0.3846\n",
            "Epoch 91/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 2.7961 - acc: 0.3879\n",
            "Epoch 92/100\n",
            "118633/118633 [==============================] - 137s 1ms/step - loss: 2.7965 - acc: 0.3894\n",
            "Epoch 93/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 2.8260 - acc: 0.3887\n",
            "Epoch 94/100\n",
            "118633/118633 [==============================] - 135s 1ms/step - loss: 2.7935 - acc: 0.3919\n",
            "Epoch 95/100\n",
            "113024/118633 [===========================>..] - ETA: 6s - loss: 3.0228 - acc: 0.3685"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAS7VZse3FIC"
      },
      "source": [
        "<a id=section605></a>\n",
        "### e. Save the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F85vCF63PGgo"
      },
      "source": [
        "- Here, we use the **Keras model API** to save the model to the file **‘model.h5‘** in the current working directory.\n",
        "\n",
        "- Later, when we **load the model** to make predictions.\n",
        "- We will also need the **mapping of words** to **integers**. \n",
        "- This is in the **Tokenizer object**, and we can save that too **using Pickle**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TG1hkckPMQg"
      },
      "source": [
        "# save the model to file\n",
        "model.save('model.h5')\n",
        "# save the tokenizer\n",
        "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTVXdfzw3FIN"
      },
      "source": [
        "<a id=section7></a>\n",
        "## 7. Use Language model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIv8u9Tz3FIP"
      },
      "source": [
        "- In this case, we can use the model to generate **new sequences of text** that have the same **statistical properties** as the source text.\n",
        "\n",
        "- We will start by **loading** the **training sequences** again.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmLOxH41PQlx"
      },
      "source": [
        "<a id=section701></a>\n",
        "### a. Load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwtQz4N3PZ-1"
      },
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, 'r')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "# load cleaned text sequences\n",
        "in_filename = 'republic_sequences.txt'\n",
        "doc = load_doc(in_filename)\n",
        "lines = doc.split('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDiucNmFPd02"
      },
      "source": [
        "[](http://)\n",
        "\n",
        "- We need the text so that we can choose a **source sequence** as input to the model for generating a **new sequence of text**.\n",
        "\n",
        "- The model will require **50 words** as **input**.\n",
        "\n",
        "- Later, we will need to specify the **expected length of input**.\n",
        "- We can determine this from the **input sequences** by **calculating the length** of one line of the loaded data and **subtracting** **1** for the **expected output** word that is also on the same line.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_uNuSY7PhVP"
      },
      "source": [
        "seq_length = len(lines[0].split()) - 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUEry7f1PjtJ"
      },
      "source": [
        "<a id=section702></a>\n",
        "### b. Load Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3G2SzwJ3FId"
      },
      "source": [
        "- We can now **load the model** from file.\n",
        "\n",
        "\n",
        "- Keras provides the **load_model() function** for loading the model, ready for use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAIRVWtaPp2l"
      },
      "source": [
        "from random import randint\n",
        "from pickle import load\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# load the model\n",
        "model = load_model('model.h5')\n",
        "\n",
        "# load the tokenizer\n",
        "tokenizer = load(open('tokenizer.pkl', 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjdIpsuK3FIg"
      },
      "source": [
        "<a id=section703></a>\n",
        "### c. Generate text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKnsr61-Puwe"
      },
      "source": [
        "* The first step in generating text is **preparing a seed input**.\n",
        "\n",
        "\n",
        "* We will select a **random line** of text from the **input text** for this purpose. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvyF0mPZHcy2"
      },
      "source": [
        "from random import randint\n",
        "from pickle import load\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "# generate a sequence from a language model\n",
        "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
        "    result = list()\n",
        "    in_text = seed_text\n",
        "    # generate a fixed number of words\n",
        "    for _ in range(n_words):\n",
        "        # encode the text as integer\n",
        "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        # truncate sequences to a fixed length\n",
        "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "        # predict probabilities for each word\n",
        "        yhat = model.predict_classes(encoded, verbose=0)\n",
        "        # map predicted word index to word\n",
        "        out_word = ''\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == yhat:\n",
        "                out_word = word\n",
        "                break\n",
        "        # append to input\n",
        "        in_text += ' ' + out_word\n",
        "        result.append(out_word)\n",
        "    return ' '.join(result)\n",
        "\n",
        "\n",
        "# load the model\n",
        "model = load_model('model.h5')\n",
        "\n",
        "# load the tokenizer\n",
        "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
        "\n",
        "# select a seed text\n",
        "seed_text = lines[randint(0,len(lines))]\n",
        "print(\"seed_text:\" + '\\n')\n",
        "print(seed_text + '\\n')\n",
        "\n",
        "# generate new text\n",
        "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
        "print(\"generated_text:\" + '\\n')\n",
        "print(generated)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3SvYiMo3FIm"
      },
      "source": [
        " - We can see that the text seems reasonable. In fact, the addition of concatenation would help in interpreting the seed and the generated text. Nevertheless, the generated text gets the right kind of words in the right kind of order.\n",
        "\n",
        " - Try running the example a few times to see other examples of generated text. We"
      ]
    }
  ]
}